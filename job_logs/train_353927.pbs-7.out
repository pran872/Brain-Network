mv: cannot stat 'job_logs/train.err': No such file or directory
Starting training on cx3-20-3.cx3.hpc.ic.ac.uk at Thu 24 Apr 15:13:00 BST 2025
Using config: configs/stanford_dogs/zoomvit_configs/config_zoomvit_p4.json
Traceback (most recent call last):
  File "/rds/general/user/psp20/home/Brain-Network/source/simple_cnn.py", line 513, in <module>
    main()
  File "/rds/general/user/psp20/home/Brain-Network/source/simple_cnn.py", line 448, in main
    best_loss_model, last_model, train_losses, val_losses, train_acc, val_acc = train_model(
                                                                                ^^^^^^^^^^^^
  File "/rds/general/user/psp20/home/Brain-Network/source/simple_cnn.py", line 120, in train_model
    loss.backward()
  File "/rds/general/user/psp20/home/.local/lib/python3.11/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/rds/general/user/psp20/home/.local/lib/python3.11/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/rds/general/user/psp20/home/.local/lib/python3.11/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: adaptive_avg_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation, or you can use the 'warn_only=True' option, if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation.
Finished at Thu 24 Apr 15:14:01 BST 2025

====================================
CPU Time used: 00:00:00
CPU Percent: 0%
Memory usage: 0b
Approx Power usage: 0.0
Walltime usage: 00:01:14

====================================
